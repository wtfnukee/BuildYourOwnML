{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logit(x, w):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "\n",
    "def sigmoid(h):\n",
    "    return 1. / (1 + np.exp(-h))\n",
    "\n",
    "\n",
    "class LogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y, max_iter=100, lr=0.1):\n",
    "        n, k = X.shape\n",
    "\n",
    "        if self.w is None:\n",
    "            self.w = np.random.randn(k + 1)\n",
    "\n",
    "        X_train = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for iter_num in range(max_iter):\n",
    "            z = sigmoid(logit(X_train, self.w))\n",
    "            grad = np.dot(X_train.T, (z - y)) / len(y)\n",
    "\n",
    "            self.w -= grad * lr\n",
    "\n",
    "            losses.append(self.__loss(y, z))\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n, k = X.shape\n",
    "        X_ = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        return sigmoid(logit(X_, self.w))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w\n",
    "\n",
    "    def __loss(self, y, p):\n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        return np.mean(y * np.log(p) + (1 - y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_batches(X, y, batch_size):\n",
    "    assert len(X) == len(y)\n",
    "    np.random.seed(42)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    perm = np.random.permutation(len(X))\n",
    "    n_samples = len(X) // batch_size\n",
    "    X = X[perm]\n",
    "    y = y[perm]\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        if len(X[i:i + n_samples]) < batch_size:\n",
    "            break\n",
    "        yield X[i:i + n_samples], y[i:i + n_samples]\n",
    "\n",
    "\n",
    "class SGDLogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y, epochs=10, lr=0.1, batch_size=100):\n",
    "        n, k = X.shape\n",
    "        if self.w is None:\n",
    "            np.random.seed(42)\n",
    "            # Вектор столбец в качестве весов\n",
    "            self.w = np.random.randn(k + 1)\n",
    "\n",
    "        X_train = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for X_batch, y_batch in generate_batches(X_train, y, batch_size):\n",
    "                #В X_train уже добавлен вектор 1\n",
    "\n",
    "                predictions = self._predict_proba_internal(X_batch)\n",
    "                loss = self.__loss(y_batch, predictions)\n",
    "\n",
    "                assert (np.array(loss).shape == tuple()), \"Лосс должен быть скаляром!\"\n",
    "\n",
    "                losses.append(loss)\n",
    "\n",
    "                self.w -= lr * self.grad(X_batch, y_batch, predictions)\n",
    "        return losses\n",
    "\n",
    "    def grad(self, X_batch, y_batch, predictions):\n",
    "        grad_basic = (predictions - y_batch)[:, np.newaxis] * X_batch\n",
    "        grad_basic = grad_basic.sum(axis=0)\n",
    "        return grad_basic\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n, k = X.shape\n",
    "        X_ = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        return sigmoid(logit(X_, self.w))\n",
    "\n",
    "    def _predict_proba_internal(self, X):\n",
    "        return sigmoid(logit(X, self.w))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w.copy()\n",
    "\n",
    "    def __loss(self, y, p):\n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def soft_sign(x, eps=1e-7):\n",
    "    if abs(x) > eps:\n",
    "        return np.sign(x)\n",
    "    return x / eps\n",
    "\n",
    "\n",
    "np_soft_sign = np.vectorize(soft_sign)\n",
    "\n",
    "\n",
    "class ElasticLogisticRegression(LogisticRegression):\n",
    "    def __init__(self, l1_coef, l2_coef):\n",
    "        super().__init__()\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.w = None\n",
    "\n",
    "    def grad(self, X_batch, y_batch, predictions):\n",
    "        grad_basic = (predictions - y_batch)[:, np.newaxis] * X_batch\n",
    "        grad_basic = grad_basic.sum(axis=0)\n",
    "\n",
    "        grad_l1 = self.l1_coef * np.sign(self.w)\n",
    "        grad_l2 = 2 * self.l2_coef * self.w\n",
    "\n",
    "        grad_l1[0] = 0\n",
    "        grad_l2[0] = 0\n",
    "\n",
    "        assert grad_l1[0] == grad_l2[0] == 0, \"Bias в регуляризационные слагаемые не входит!\"\n",
    "        assert grad_basic.shape == grad_l1.shape == grad_l2.shape == (\n",
    "            X_batch.shape[1],), \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
    "\n",
    "        return grad_basic + grad_l1 + grad_l2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
