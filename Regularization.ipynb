{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regularization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RidgeRegression(LinearRegression):\n",
    "    def __init__(self, alpha=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n, m = X.shape\n",
    "        X_train = X\n",
    "\n",
    "        if self.fit_intercept:\n",
    "            X_train = np.hstack((X, np.ones((n, 1))))\n",
    "\n",
    "        lambdaI = self.alpha * np.eye(X_train.shape[1])\n",
    "        if self.fit_intercept:\n",
    "            lambdaI[-1, -1] = 0\n",
    "\n",
    "        self.w = np.linalg.inv(X_train.T @ X_train + lambdaI) @ X_train.T @ y\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGDRidge(SGDLinearRegression):\n",
    "    def __init__(self, alpha=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)  # передает именные параметры родительскому конструктору\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _calc_gradient(self, X, y, y_pred):\n",
    "        # Главное отличие в SGD - это использование подвыборки для шага оптимизации\n",
    "        inds = np.random.choice(np.arange(X.shape[0]), size=self.n_sample, replace=False)\n",
    "\n",
    "        lambdaI = self.alpha * np.eye(self.w.shape[0])\n",
    "        if self.fit_intercept:\n",
    "            lambdaI[-1, -1] = 0\n",
    "\n",
    "        grad = 2 * (X[inds].T @ X[inds] / self.n_sample + lambdaI) @ self.w\n",
    "        grad -= 2 * X[inds].T @ y[inds] / self.n_sample\n",
    "\n",
    "        return grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def soft_sign(x, eps=1e-7):\n",
    "    if abs(x) > eps:\n",
    "        return np.sign(x)\n",
    "    return x / eps\n",
    "\n",
    "np_soft_sign = np.vectorize(soft_sign)\n",
    "\n",
    "class SGDLasso(SGDLinearRegression):\n",
    "    def __init__(self, alpha=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # передает именные параметры родительскому конструктору\n",
    "        self.w = None\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def _calc_gradient(self, X, y, y_pred):\n",
    "        # Главное отличие в SGD - это использование подвыборки для шага оптимизации\n",
    "        inds = np.random.choice(np.arange(X.shape[0]), size=self.n_sample, replace=False)\n",
    "\n",
    "        signw = np_soft_sign(self.w)\n",
    "        if self.fit_intercept:\n",
    "            signw[-1] = 0\n",
    "\n",
    "        grad = X[inds].T @ (y_pred[inds] - y[inds])[:, np.newaxis] / self.n_sample\n",
    "        grad += self.alpha * signw[:, np.newaxis]\n",
    "\n",
    "        return grad.flatten()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
